{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pickle\n\nBOARD_ROWS = 3\nBOARD_COLS = 3\nBOARD_SIZE = BOARD_ROWS * BOARD_COLS\n\nclass State:\n    def __init__(self):\n        # the board is represented by an n * n array,\n        # 1 represents a chessman of the player who moves first,\n        # -1 represents a chessman of another player\n        # 0 represents an empty position\n        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n        self.winner = None\n        self.hash_val = None\n        self.end = None\n\n    # compute the hash value for one state, it's unique\n    def hash(self):\n        if self.hash_val is None:\n            self.hash_val = 0\n            for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\n                if i == -1:\n                    i = 2\n                self.hash_val = self.hash_val * 3 + i\n        return int(self.hash_val)\n\n    # check whether a player has won the game, or it's a tie\n    def is_end(self):\n        if self.end is not None:\n            return self.end\n        results = []\n        # check row\n        for i in range(0, BOARD_ROWS):\n            results.append(np.sum(self.data[i, :]))\n        # check columns\n        for i in range(0, BOARD_COLS):\n            results.append(np.sum(self.data[:, i]))\n\n        # check diagonals\n        results.append(0)\n        for i in range(0, BOARD_ROWS):\n            results[-1] += self.data[i, i]\n        results.append(0)\n        for i in range(0, BOARD_ROWS):\n            results[-1] += self.data[i, BOARD_ROWS - 1 - i]\n\n        for result in results:\n            if result == 3:\n                self.winner = 1\n                self.end = True\n                return self.end\n            if result == -3:\n                self.winner = -1\n                self.end = True\n                return self.end\n\n        # whether it's a tie\n        sum = np.sum(np.abs(self.data))\n        if sum == BOARD_ROWS * BOARD_COLS:\n            self.winner = 0\n            self.end = True\n            return self.end\n\n        # game is still going on\n        self.end = False\n        return self.end\n\n    # @symbol: 1 or -1\n    # put chessman symbol in position (i, j)\n    def next_state(self, i, j, symbol):\n        new_state = State()\n        new_state.data = np.copy(self.data)\n        new_state.data[i, j] = symbol\n        return new_state\n\n    # print the board\n    def print(self):\n        for i in range(0, BOARD_ROWS):\n            print('-------------')\n            out = '| '\n            for j in range(0, BOARD_COLS):\n                if self.data[i, j] == 1:\n                    token = '*'\n                if self.data[i, j] == 0:\n                    token = '0'\n                if self.data[i, j] == -1:\n                    token = 'x'\n                out += token + ' | '\n            print(out)\n        print('-------------')\n\ndef get_all_states_impl(current_state, current_symbol, all_states):\n    for i in range(0, BOARD_ROWS):\n        for j in range(0, BOARD_COLS):\n            if current_state.data[i][j] == 0:\n                newState = current_state.next_state(i, j, current_symbol)\n                newHash = newState.hash()\n                if newHash not in all_states.keys():\n                    isEnd = newState.is_end()\n                    all_states[newHash] = (newState, isEnd)\n                    if not isEnd:\n                        get_all_states_impl(newState, -current_symbol, all_states)\n\ndef get_all_states():\n    current_symbol = 1\n    current_state = State()\n    all_states = dict()\n    all_states[current_state.hash()] = (current_state, current_state.is_end())\n    get_all_states_impl(current_state, current_symbol, all_states)\n    return all_states\n\n# all possible board configurations\nall_states = get_all_states()\n\nclass Judger:\n    # @player1: the player who will move first, its chessman will be 1\n    # @player2: another player with a chessman -1\n    # @feedback: if True, both players will receive rewards when game is end\n    def __init__(self, player1, player2):\n        self.p1 = player1\n        self.p2 = player2\n        self.current_player = None\n        self.p1_symbol = 1\n        self.p2_symbol = -1\n        self.p1.set_symbol(self.p1_symbol)\n        self.p2.set_symbol(self.p2_symbol)\n        self.current_state = State()\n\n    def reset(self):\n        self.p1.reset()\n        self.p2.reset()\n\n    def alternate(self):\n        while True:\n            yield self.p1\n            yield self.p2\n\n    # @print: if True, print each board during the game\n    def play(self, print=False):\n        alternator = self.alternate()\n        self.reset()\n        current_state = State()\n        self.p1.set_state(current_state)\n        self.p2.set_state(current_state)\n        while True:\n            player = next(alternator)\n            if print:\n                current_state.print()\n            [i, j, symbol] = player.act()\n            next_state_hash = current_state.next_state(i, j, symbol).hash()\n            current_state, is_end = all_states[next_state_hash]\n            self.p1.set_state(current_state)\n            self.p2.set_state(current_state)\n            if is_end:\n                if print:\n                    current_state.print()\n                return current_state.winner\n\n# AI player\nclass Player:\n    # @step_size: the step size to update estimations\n    # @epsilon: the probability to explore\n    def __init__(self, step_size=0.1, epsilon=0.1):\n        self.estimations = dict()\n        self.step_size = step_size\n        self.epsilon = epsilon\n        self.states = []\n        self.greedy = []\n\n    def reset(self):\n        self.states = []\n        self.greedy = []\n\n    def set_state(self, state):\n        self.states.append(state)\n        self.greedy.append(True)\n\n    def set_symbol(self, symbol):\n        self.symbol = symbol\n        for hash_val in all_states.keys():\n            (state, is_end) = all_states[hash_val]\n            if is_end:\n                if state.winner == self.symbol:\n                    self.estimations[hash_val] = 1.0\n                elif state.winner == 0:\n                    # we need to distinguish between a tie and a lose\n                    self.estimations[hash_val] = 0.5\n                else:\n                    self.estimations[hash_val] = 0\n            else:\n                self.estimations[hash_val] = 0.5\n\n    # update value estimation\n    def backup(self):\n        # for debug\n        # print('player trajectory')\n        # for state in self.states:\n        #     state.print()\n\n        self.states = [state.hash() for state in self.states]\n\n        for i in reversed(range(len(self.states) - 1)):\n            state = self.states[i]\n            td_error = self.greedy[i] * (self.estimations[self.states[i + 1]] - self.estimations[state])\n            self.estimations[state] += self.step_size * td_error\n\n    # choose an action based on the state\n    def act(self):\n        state = self.states[-1]\n        next_states = []\n        next_positions = []\n        for i in range(BOARD_ROWS):\n            for j in range(BOARD_COLS):\n                if state.data[i, j] == 0:\n                    next_positions.append([i, j])\n                    next_states.append(state.next_state(i, j, self.symbol).hash())\n\n        ## a probability of epsilon to try a new action\n        if np.random.rand() < self.epsilon:\n            action = next_positions[np.random.randint(len(next_positions))]\n            action.append(self.symbol)\n            self.greedy[-1] = False\n            return action\n\n        ## choose action with highest value according to current estimation records\n        values = []\n        for hash, pos in zip(next_states, next_positions):\n            values.append((self.estimations[hash], pos))\n        np.random.shuffle(values)\n        values.sort(key=lambda x: x[0], reverse=True)\n        action = values[0][1]\n        action.append(self.symbol)\n        return action\n\n    def save_policy(self):\n        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n            pickle.dump(self.estimations, f)\n\n    def load_policy(self):\n        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n            self.estimations = pickle.load(f)\n\n# human interface\n# input a number to put a chessman\n# | q | w | e |\n# | a | s | d |\n# | z | x | c |\nclass HumanPlayer:\n    def __init__(self, **kwargs):\n        self.symbol = None\n        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n        self.state = None\n        return\n\n    def reset(self):\n        return\n\n    def set_state(self, state):\n        self.state = state\n\n    def set_symbol(self, symbol):\n        self.symbol = symbol\n        return\n\n    def backup(self, _):\n        return\n\n    def act(self):\n        self.state.print()\n        key = input(\"Input your position:\")\n        data = self.keys.index(key)\n        i = data // int(BOARD_COLS)\n        j = data % BOARD_COLS\n        return (i, j, self.symbol)\n\ndef train(epochs):\n    player1 = Player(epsilon=0.01)\n    player2 = Player(epsilon=0.01)\n    judger = Judger(player1, player2)\n    player1_win = 0.0\n    player2_win = 0.0\n    for i in range(1, epochs + 1):\n        winner = judger.play(print=False)\n        if winner == 1:\n            player1_win += 1\n        if winner == -1:\n            player2_win += 1\n        print('Epoch %d, player 1 win %.02f, player 2 win %.02f' % (i, player1_win / i, player2_win / i))\n        player1.backup()\n        player2.backup()\n        judger.reset()\n    player1.save_policy()\n    player2.save_policy()\n\ndef compete(turns):\n    player1 = Player(epsilon=0)\n    player2 = Player(epsilon=0)\n    judger = Judger(player1, player2)\n    player1.load_policy()\n    player2.load_policy()\n    player1_win = 0.0\n    player2_win = 0.0\n    for i in range(0, turns):\n        winner = judger.play()\n        if winner == 1:\n            player1_win += 1\n        if winner == -1:\n            player2_win += 1\n        judger.reset()\n    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))\n\n# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n# So we test whether the AI can guarantee at least a tie if it goes second.\ndef play():\n    while True:\n        player1 = HumanPlayer()\n        player2 = Player(epsilon=0)\n        judger = Judger(player1, player2)\n        player2.load_policy()\n        winner = judger.play()\n        if winner == player2.symbol:\n            print(\"You lose!\")\n        elif winner == player1.symbol:\n            print(\"You win!\")\n        else:\n            print(\"It is a tie!\")\n\nif __name__ == '__main__':\n    train(int(1e5))\n    compete(int(1e3))\n    play()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}